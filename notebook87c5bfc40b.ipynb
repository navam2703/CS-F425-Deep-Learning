{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## LOAD DATA and IMPORTING LIBRARIES","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:15:21.473132Z","iopub.execute_input":"2021-10-29T13:15:21.473467Z","iopub.status.idle":"2021-10-29T13:15:21.478595Z","shell.execute_reply.started":"2021-10-29T13:15:21.473432Z","shell.execute_reply":"2021-10-29T13:15:21.477662Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/fashionmnist/fashion-mnist_train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/fashionmnist/fashion-mnist_test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:15:21.480083Z","iopub.execute_input":"2021-10-29T13:15:21.480365Z","iopub.status.idle":"2021-10-29T13:15:27.237789Z","shell.execute_reply.started":"2021-10-29T13:15:21.480327Z","shell.execute_reply":"2021-10-29T13:15:27.236907Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:15:27.239376Z","iopub.execute_input":"2021-10-29T13:15:27.239612Z","iopub.status.idle":"2021-10-29T13:15:29.925970Z","shell.execute_reply.started":"2021-10-29T13:15:27.239583Z","shell.execute_reply":"2021-10-29T13:15:29.925068Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"x_train = train_df.iloc[:, 1:785]\ny_train = train_df.iloc[:, [0]]\n\nx_test = test_df.iloc[:, 1:785]\ny_test = test_df.iloc[:, [0]]","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:15:29.927357Z","iopub.execute_input":"2021-10-29T13:15:29.927620Z","iopub.status.idle":"2021-10-29T13:15:29.934659Z","shell.execute_reply.started":"2021-10-29T13:15:29.927590Z","shell.execute_reply":"2021-10-29T13:15:29.933932Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:15:29.936297Z","iopub.execute_input":"2021-10-29T13:15:29.936631Z","iopub.status.idle":"2021-10-29T13:15:29.953567Z","shell.execute_reply.started":"2021-10-29T13:15:29.936602Z","shell.execute_reply":"2021-10-29T13:15:29.952456Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## MODEL 1\n","metadata":{}},{"cell_type":"code","source":"model = keras.Sequential([\n    keras.layers.Dense(units=32, activation='relu', input_shape=[784]),\n    keras.layers.Dense(units=16, activation='relu'),\n    keras.layers.Dense(units=10, activation='softmax'),\n])\nmodel.compile(optimizer='adam',\n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train, \n                    epochs=180, batch_size = 5000, \n                    validation_split = 0.2\n                    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nmodel.evaluate(x_test, y_test, batch_size=2000)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:15:29.956448Z","iopub.execute_input":"2021-10-29T13:15:29.956968Z","iopub.status.idle":"2021-10-29T13:16:55.437511Z","shell.execute_reply.started":"2021-10-29T13:15:29.956923Z","shell.execute_reply":"2021-10-29T13:16:55.436444Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## MODEL2\n- single layer 1024 cells\n- epochs = 50\n- activation = relu \n- accuracy not converging indicating we might need more number of epochs\n- _slight overfitting, possible that deeper networks will model better_\n- _accuracy on training data = 90.19%, accuracy on testing data = 85.77%","metadata":{}},{"cell_type":"code","source":"model2 = keras.Sequential([\n    keras.layers.Dense(units=1024, activation='relu', input_shape=[784]),\n    keras.layers.Dense(units=10, activation='softmax'),\n])\nmodel2.compile(optimizer='adam',\n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\n\nhistory = model2.fit(x_train, y_train, \n                    epochs= 50, batch_size = 5000, \n                    validation_split = 0.2\n                    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nmodel2.evaluate(x_test, y_test, batch_size=2000)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:16:55.439280Z","iopub.execute_input":"2021-10-29T13:16:55.439605Z","iopub.status.idle":"2021-10-29T13:18:22.318085Z","shell.execute_reply.started":"2021-10-29T13:16:55.439562Z","shell.execute_reply":"2021-10-29T13:18:22.316858Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## MODEL 3\n- attempting at creating a deeper network with more number of nodes per hidden layer as compared to model 1\n- activation = relu \n- _train accuracy = 89.24%, test accuracy = 84.18% -> overfitting_\n- deeper net might be more useful","metadata":{}},{"cell_type":"code","source":"model3 = keras.Sequential([\n    #hidden layers\n    keras.layers.Dense(units= 128, activation='relu', input_shape=[784]),\n    keras.layers.Dense(units= 64, activation='relu'),\n    #output layer\n    keras.layers.Dense(units=10, activation='softmax'),\n])\nmodel3.compile(optimizer='adam',\n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\n\nhistory = model3.fit(x_train, y_train, \n                    epochs=100, batch_size = 5000, \n                    validation_split = 0.2\n                    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nmodel3.evaluate(x_test, y_test, batch_size=2000)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:18:22.319768Z","iopub.execute_input":"2021-10-29T13:18:22.320125Z","iopub.status.idle":"2021-10-29T13:19:10.752900Z","shell.execute_reply.started":"2021-10-29T13:18:22.320068Z","shell.execute_reply":"2021-10-29T13:19:10.751991Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## MODEL 4\n- 3 hidden layers - 128, 64, 32 units\n- accuracy drops\n- activation = relu \n- _train accuracy = 92.32%, test accuracy = 85.37%_","metadata":{}},{"cell_type":"code","source":"model4 = keras.Sequential([\n    #hidden layers\n    keras.layers.Dense(units= 128, activation='relu', input_shape=[784]),\n    keras.layers.Dense(units= 64, activation='relu'),\n    keras.layers.Dense(units= 32, activation='relu'),\n    #output layer\n    keras.layers.Dense(units=10, activation='softmax'),\n])\nmodel4.compile(optimizer='adam',\n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\n\nhistory = model4.fit(x_train, y_train, \n                    epochs=180, batch_size = 5000, \n                    validation_split = 0.2\n                    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nmodel4.evaluate(x_test, y_test, batch_size=2000)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:19:10.754344Z","iopub.execute_input":"2021-10-29T13:19:10.755262Z","iopub.status.idle":"2021-10-29T13:20:35.775390Z","shell.execute_reply.started":"2021-10-29T13:19:10.755212Z","shell.execute_reply":"2021-10-29T13:20:35.774413Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## MODEL 5\n- 2 layers = 256, 128\n- activation = relu \n- _Overfitting, train accuracy : 94.32%, test accuracy : 84.88%","metadata":{}},{"cell_type":"code","source":"model5 = keras.Sequential([\n    #hidden layers\n    keras.layers.Dense(units= 256, activation='relu', input_shape=[784]),\n    keras.layers.Dense(units= 128, activation='relu'),\n    #output layer\n    keras.layers.Dense(units=10, activation='softmax'),\n])\nmodel5.compile(optimizer='adam',\n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\n\nhistory = model5.fit(x_train, y_train, \n                    epochs=100, batch_size = 5000, \n                    validation_split = 0.2\n                    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nmodel5.evaluate(x_test, y_test, batch_size=2000)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:20:35.777311Z","iopub.execute_input":"2021-10-29T13:20:35.777639Z","iopub.status.idle":"2021-10-29T13:21:49.189963Z","shell.execute_reply.started":"2021-10-29T13:20:35.777606Z","shell.execute_reply":"2021-10-29T13:21:49.187912Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## MODEL 6\n- making use of sigmoid activations\n- _train accuracy = 91.21%, test accuracy = 87.60% (higher accuracy as compared to ReLU activation functions)","metadata":{}},{"cell_type":"code","source":"model6 = keras.Sequential([\n    #hidden layers\n    keras.layers.Dense(units= 128, activation='relu', input_shape=[784]),\n    keras.layers.Dense(units= 64, activation='sigmoid'),\n    keras.layers.Dense(units= 32, activation='sigmoid'),\n    #output layer\n    keras.layers.Dense(units=10, activation='softmax'),\n])\nmodel6.compile(optimizer='adam',\n                loss= 'sparse_categorical_crossentropy',\n                metrics=['accuracy'])\n\nhistory = model6.fit(x_train, y_train, \n                    epochs=100, batch_size = 5000, \n                    validation_split = 0.2\n                    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nmodel6.evaluate(x_test, y_test, batch_size=2000)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:21:49.192359Z","iopub.execute_input":"2021-10-29T13:21:49.193124Z","iopub.status.idle":"2021-10-29T13:22:38.068533Z","shell.execute_reply.started":"2021-10-29T13:21:49.193080Z","shell.execute_reply":"2021-10-29T13:22:38.067685Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## MODEL 7\n- previous combo seemed to work so we try to increase number of nodes (in an attempt to increase train set accuracy) and to counter the overfitting we further add some regularization\n- _train accuracy = 93.44%, test accuracy = 88.94%_","metadata":{}},{"cell_type":"code","source":"model7 = keras.Sequential([\n    #hidden layers\n    keras.layers.Dense(units= 128, activation='relu', input_shape=[784]),\n    keras.layers.Dense(units= 64, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(0.001)),\n    keras.layers.Dense(units= 64, activation='sigmoid', kernel_regularizer= keras.regularizers.L1L2(l1=0.001, l2=0.001)),\n    #output layer\n    keras.layers.Dense(units=10, activation='softmax'),\n])\nmodel7.compile(optimizer='adam',\n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\n\nhistory = model7.fit(x_train, y_train, \n                    epochs=100, batch_size = 5000, \n                    validation_split = 0.2\n                    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nmodel7.evaluate(x_test, y_test, batch_size=2000)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:22:38.069883Z","iopub.execute_input":"2021-10-29T13:22:38.070185Z","iopub.status.idle":"2021-10-29T13:23:29.475216Z","shell.execute_reply.started":"2021-10-29T13:22:38.070143Z","shell.execute_reply":"2021-10-29T13:23:29.474173Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## MODEL 8\n- A deeper model with 512, 64, 32 nodes in the hidden layers and added regularization while maintaining a sigmoid activation function\n- Also, validation and test accuracy difference is quite big signifying overfitting and thus Dropout should be used.\n- _train accuracy = 97.17%, test accuracy = 90.02%_","metadata":{}},{"cell_type":"code","source":"model8 = keras.Sequential([\n    #hidden layers\n    keras.layers.Dense(units= 512, activation='relu', input_shape=[784]),\n    keras.layers.Dense(units= 64, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(0.001)),\n    keras.layers.Dense(units= 32, activation='sigmoid', kernel_regularizer= keras.regularizers.L1L2(l1=0.001, l2=0.001)),\n    #output layer\n    keras.layers.Dense(units=10, activation='softmax'),\n])\nmodel8.compile(optimizer='adam', \n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\n\nhistory = model8.fit(x_train, y_train, \n                    epochs=100, batch_size = 5000, \n                    validation_split = 0.2\n                    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nmodel8.evaluate(x_test, y_test, batch_size=2000)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:23:29.476824Z","iopub.execute_input":"2021-10-29T13:23:29.477103Z","iopub.status.idle":"2021-10-29T13:25:53.367988Z","shell.execute_reply.started":"2021-10-29T13:23:29.477069Z","shell.execute_reply":"2021-10-29T13:25:53.367347Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## MODEL 9\n- KL divergence loss\n- poor performance\n- very small decrease in training and cross validation loss and almost no increase in accuracy\n- thus we continue with categorical cross entropy loss function\n- _train accuracy = 10.18%, test accuracy = 11.69%_","metadata":{}},{"cell_type":"code","source":"model9 = keras.Sequential([\n    #hidden layers\n    keras.layers.Dense(units= 512, activation='relu', input_shape=[784]),\n    keras.layers.Dense(units= 64, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(0.001)),\n    keras.layers.Dense(units= 32, activation='sigmoid', kernel_regularizer= keras.regularizers.L1L2(l1=0.001, l2=0.001)),\n    #output layer\n    keras.layers.Dense(units=10, activation='softmax'),\n])\nmodel9.compile(optimizer='adam', \n                loss= keras.losses.KLDivergence(),\n                metrics=['accuracy'])\n\nhistory = model9.fit(x_train, y_train, \n                    epochs= 50, batch_size = 5000, \n                    validation_split = 0.2\n                    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nmodel9.evaluate(x_test, y_test, batch_size=2000)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:25:53.369946Z","iopub.execute_input":"2021-10-29T13:25:53.370460Z","iopub.status.idle":"2021-10-29T13:26:48.710863Z","shell.execute_reply.started":"2021-10-29T13:25:53.370400Z","shell.execute_reply":"2021-10-29T13:26:48.709968Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## MODEL 10\n- checking if small neural nets with less number of activation nodes but more layers provide simlilar performance\n- 4 hidden layers with 32 nodes each and a ReLU activation function\n- _train accuracy = 85.92%, test accuracy = 84.02%_","metadata":{}},{"cell_type":"code","source":"model10 = keras.Sequential([\n    #hidden layers\n    keras.layers.Dense(units= 32, activation='relu', input_shape=[784]),\n    keras.layers.Dense(units= 32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n    keras.layers.Dense(units= 32, activation='relu', kernel_regularizer= keras.regularizers.l2(0.001)),\n    keras.layers.Dense(units= 32, activation='relu', kernel_regularizer= keras.regularizers.l2(0.001)),\n    \n    #output layer\n    keras.layers.Dense(units=10, activation='softmax'),\n])\nmodel10.compile(optimizer='adam', \n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\n\nhistory = model10.fit(x_train, y_train, \n                    epochs=100, batch_size = 5000, \n                    validation_split = 0.2\n                    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nmodel10.evaluate(x_test, y_test, batch_size=2000)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:26:48.712823Z","iopub.execute_input":"2021-10-29T13:26:48.713166Z","iopub.status.idle":"2021-10-29T13:27:21.661671Z","shell.execute_reply.started":"2021-10-29T13:26:48.713120Z","shell.execute_reply":"2021-10-29T13:27:21.660692Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## MODEL 11\n- we have been getting similar performance in models 5-8. Hence we may attempt to use dropout regularization after the first 2 hidden layers in an attampt to reduce overfitting\n- Dropout randomly sets some of the outgoing edges from neurons to zero so as to avoid overfitting\n- _Success!_\n- _Dropout 0.2 after 1st 2 hidden layers_\n- _train accuracy = 94.3%, cross_val accuracy = 93.19%, test accuracy = 89.74%_","metadata":{}},{"cell_type":"code","source":"model11 = keras.Sequential([\n    #hidden layers\n    keras.layers.Dense(units= 512, activation='relu', input_shape=[784]),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 64, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(0.001)),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 32, activation='sigmoid', kernel_regularizer= keras.regularizers.L1L2(l1=0.001, l2=0.001)),\n    #output layer\n    keras.layers.Dense(units=10, activation='softmax'),\n])\nmodel11.compile(optimizer='adam', \n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\n\nhistory = model11.fit(x_train, y_train, \n                    epochs=100, batch_size = 5000, \n                    validation_split = 0.2\n                    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nmodel11.evaluate(x_test, y_test, batch_size=2000)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:27:21.663039Z","iopub.execute_input":"2021-10-29T13:27:21.663318Z","iopub.status.idle":"2021-10-29T13:29:24.777914Z","shell.execute_reply.started":"2021-10-29T13:27:21.663286Z","shell.execute_reply":"2021-10-29T13:29:24.777284Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## MODEL 12\n- A deeper neural net with 512,256,256,128,64 nodes in the hidden layers with dropout on the first 4 layers\n- Activation- ReLU\n- We see that due to a deeper neural network, the weights are taking a longer time to converge, hence increasing the number of epochs may help\n- _train accuracy = 94.3%, cross_val accuracy = 89.94%, test accuracy = 88.59%_","metadata":{}},{"cell_type":"code","source":"model12 = keras.Sequential([\n    #hidden layers\n    keras.layers.Dense(units= 512, activation='relu', input_shape=[784]),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 64, activation='relu', kernel_regularizer= keras.regularizers.L1L2(l1=0.001, l2=0.001)),\n    #output layer\n    keras.layers.Dense(units=10, activation='softmax'),\n])\nmodel12.compile(optimizer='adam', \n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\n\nhistory = model12.fit(x_train, y_train, \n                    epochs=100, batch_size = 5000, \n                    validation_split = 0.2\n                    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nmodel12.evaluate(x_test, y_test, batch_size=2000)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:29:24.779745Z","iopub.execute_input":"2021-10-29T13:29:24.780255Z","iopub.status.idle":"2021-10-29T13:32:44.040389Z","shell.execute_reply.started":"2021-10-29T13:29:24.780195Z","shell.execute_reply":"2021-10-29T13:32:44.039462Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## MODEL 13\n- Similar architecture as Model 12, number of epochs increased to 400 \n- Activation- ReLU\n- _train accuracy = 97.75%,test accuracy = 89.61%, validation accuracy- 89.52%\n- Validation Set accuracy hardly improves after the 100th epoch while training set accuracy keeps on increasing\n- thus we observe that as we keep on increasing the complexity of the neural network, the difference between train and validation accuracy keeps increasing indicating overfitting.\n","metadata":{}},{"cell_type":"code","source":"model13 = keras.Sequential([\n    #hidden layers\n    keras.layers.Dense(units= 512, activation='relu', input_shape=[784]),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 64, activation='relu', kernel_regularizer= keras.regularizers.L1L2(l1=0.001, l2=0.001)),\n    #output layer\n    keras.layers.Dense(units=10, activation='softmax'),\n])\nmodel13.compile(optimizer='adam', \n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\n\nhistory = model13.fit(x_train, y_train, \n                    epochs=400, batch_size = 5000, \n                    validation_split = 0.2\n                    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nmodel13.evaluate(x_test, y_test, batch_size=2000)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:32:44.041915Z","iopub.execute_input":"2021-10-29T13:32:44.042173Z","iopub.status.idle":"2021-10-29T13:45:55.178798Z","shell.execute_reply.started":"2021-10-29T13:32:44.042141Z","shell.execute_reply":"2021-10-29T13:45:55.177896Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Model 14\n\n- 3 hidden layers with 512,256,64 nodes\n- Tanh activation function used with droupout in first 2 layers\n- Train Accuracy- 84.07% Test Accuracy- 85.25%\n- This is lesser than sigmoid and relu activation functions.","metadata":{}},{"cell_type":"code","source":"model14 = keras.Sequential([\n    #hidden layers\n    keras.layers.Dense(units= 512, activation='tanh', input_shape=[784]),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 256, activation='tanh', kernel_regularizer=keras.regularizers.l2(0.001)),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 64, activation='tanh', kernel_regularizer= keras.regularizers.L1L2(l1=0.001, l2=0.001)),\n    #output layer\n    keras.layers.Dense(units=10, activation='softmax'),\n])\nmodel14.compile(optimizer='adam', \n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\n\nhistory = model14.fit(x_train, y_train, \n                    epochs=100, batch_size = 5000, \n                    validation_split = 0.2\n                    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nmodel14.evaluate(x_test, y_test, batch_size=2000)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:45:55.180283Z","iopub.execute_input":"2021-10-29T13:45:55.180530Z","iopub.status.idle":"2021-10-29T13:48:38.132124Z","shell.execute_reply.started":"2021-10-29T13:45:55.180500Z","shell.execute_reply":"2021-10-29T13:48:38.130979Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Model 15\n\n- Trying a neural network with 5 hidden layers with 64 nodes at each of the hidden layer.\n- Training accuracy- 81.87% Test accuracy -83.04%\n- Model is not able to converge even after 400 epochs due to so many hidden layers.","metadata":{}},{"cell_type":"code","source":"model15 = keras.Sequential([\n    #hidden layers\n    keras.layers.Dense(units= 64, activation='sigmoid', input_shape=[784]),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 64, activation='sigmoid'),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 64, activation='sigmoid'),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 64, activation='sigmoid'),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 64, activation='sigmoid'),\n    keras.layers.Dropout(0.2),\n    #output layer\n    keras.layers.Dense(units=10, activation='softmax'),\n])\nmodel15.compile(optimizer='adam', \n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\n\nhistory = model15.fit(x_train, y_train, \n                    epochs=400, batch_size = 5000, \n                    validation_split = 0.2\n                    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nmodel15.evaluate(x_test, y_test, batch_size=2000)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:48:38.133754Z","iopub.execute_input":"2021-10-29T13:48:38.134013Z","iopub.status.idle":"2021-10-29T13:52:11.892954Z","shell.execute_reply.started":"2021-10-29T13:48:38.133979Z","shell.execute_reply":"2021-10-29T13:52:11.891764Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Model 16\n- tanh activation function and KL divergence loss function\n- 128,64,32 nodes in the hidden layers\n- Training accuracy 10.13% Test accuracy 5.57%\n- Extremely low accuracy with a lot of spikes in the graph.","metadata":{}},{"cell_type":"code","source":"model16 = keras.Sequential([\n    #hidden layers\n    keras.layers.Dense(units= 128, activation='tanh', input_shape=[784]),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 64, activation='tanh'),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(units= 32, activation='tanh'),\n    keras.layers.Dropout(0.2),\n    #output layer\n    keras.layers.Dense(units=10, activation='softmax'),\n])\nmodel16.compile(optimizer='adam', \n                loss=keras.losses.KLDivergence(),\n                metrics=['accuracy'])\n\nhistory = model16.fit(x_train, y_train, \n                    epochs=100, batch_size = 5000,   \n                    validation_split = 0.2\n                    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n\nmodel16.evaluate(x_test, y_test, batch_size=2000)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T13:52:11.894743Z","iopub.execute_input":"2021-10-29T13:52:11.895291Z","iopub.status.idle":"2021-10-29T13:53:10.246882Z","shell.execute_reply.started":"2021-10-29T13:52:11.895241Z","shell.execute_reply":"2021-10-29T13:53:10.246049Z"},"trusted":true},"execution_count":24,"outputs":[]}]}